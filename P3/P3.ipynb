{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126e784-22cc-4b8f-915e-a5f1cc18721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from util import *\n",
    "from models import *\n",
    "from classification import *\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57c135-3a39-4f86-93a4-ed9825117075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', type=str, default='Wallpaper', help='Dataset to use (Taiji or Wallpaper)')\n",
    "    parser.add_argument('--save_dir', type=str, default='results', help='Directory to save results')\n",
    "    parser.add_argument('--data_root', type=str, default='data', help='Directory to save results')\n",
    "    parser.add_argument('--num_epochs', type=int, default=1, help='Number of epochs to train')\n",
    "    parser.add_argument('--device', type=str, default='cuda', help='Device to use (cuda or cpu)')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='Batch size')\n",
    "    parser.add_argument('--seed', type=int, default=2023, help='Random seed')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\n",
    "    parser.add_argument('--log_dir', type=str, default='logs', help='Directory to save logs')\n",
    "    parser.add_argument('--log_interval', type=int, default=1, help='Print loss every log_interval epochs, feel free to change')\n",
    "    parser.add_argument('--train' , action='store_true', help='Train the model')\n",
    "    parser.add_argument('--save_model', action='store_true', help='Save the model')\n",
    "    parser.add_argument('--baseline', action = 'store_true', help = 'Basline model configuiration')\n",
    "    parser.add_argument('--model1', action = 'store_true', help = 'Model-1 configuiration, built on top of baseline')\n",
    "    parser.add_argument('--model2', action = 'store_true', help = 'Model-2 configuiration, built on top of baseline')\n",
    "    parser.add_argument('--num_subs', type=int, default=10, help='Number of subjects to train and test on')\n",
    "    parser.add_argument('--fp_size', type=str, default='lod4', help='Size of the fingerprint to use (lod4 or full)')\n",
    "    parser.add_argument('--img_size', type=int, default=128, help='Size of image to be resized to')\n",
    "    parser.add_argument('--test_set', type=str, default='test', help='Test set to use (test or test_challenge)')\n",
    "    parser.add_argument('--aug_train', action='store_true', help='Use augmented training data')\n",
    "    return parser.parse_args(args = [])\n",
    "args = arg_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b62b8-aef5-4a34-bf70-6de47ed2bb0b",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39d79d-c8f3-4540-8be4-4eb998e5db21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_subs = args.num_subs\n",
    "num_forms = 46\n",
    "sub_train_acc = np.zeros(num_subs)\n",
    "sub_class_train = np.zeros((num_subs, num_forms))\n",
    "sub_test_acc = np.zeros(num_subs)\n",
    "sub_class_test = np.zeros((num_subs, num_forms))\n",
    "overall_train_mat = np.zeros((num_forms, 1))\n",
    "overall_test_mat = np.zeros((num_forms, 1))\n",
    "\n",
    "if not os.path.exists(os.path.join(args.save_dir, 'Taiji', args.fp_size)):\n",
    "    os.makedirs(os.path.join(args.save_dir, 'Taiji', args.fp_size))\n",
    "if args.device == 'cuda':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd951a41-f529-4769-905c-0a65f039a3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('\\n\\nTraining subject: {}'.format(1))\n",
    "\n",
    "train_data = TaijiData(data_dir='data', subject= 1, split='train')\n",
    "test_data = TaijiData(data_dir ='data', subject= 1, split='test')\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "model = MLP(input_dim = train_data.data_dim, hidden_dim = 1024, output_dim = num_forms).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train + test the model\n",
    "model, train_losses, per_epoch_train_acc, train_preds, train_targets \\\n",
    "                = train(model, train_loader, optimizer, criterion, args.num_epochs, args.log_interval, device)\n",
    "test_loss, test_acc, test_pred, test_targets = test(model, test_loader, device, criterion)\n",
    "\n",
    "# Print accs to three decimal places\n",
    "sub_train_acc[0] = per_epoch_train_acc[-1]\n",
    "sub_test_acc[0] = test_acc\n",
    "print(f'Subject {1} Train Accuracy: {per_epoch_train_acc[-1]*100:.3f}')\n",
    "print(f'Subject {1} Test Accuracy: {test_acc*100:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd00905-2adf-4034-91c7-7c5fd11a9831",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37e6c1-5f39-4e2e-aba9-96fc5eb13d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 17\n",
    "if args.device == 'cuda':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "data_root = os.path.join(args.data_root, 'Wallpaper')\n",
    "if not os.path.exists(os.path.join(args.save_dir, 'Wallpaper', args.test_set)):\n",
    "    os.makedirs(os.path.join(args.save_dir, 'Wallpaper', args.test_set))\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((args.img_size, args.img_size)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, )),\n",
    "])\n",
    "train_dataset = ImageFolder(os.path.join(data_root, 'train'), transform=transform)\n",
    "test_dataset = ImageFolder(os.path.join(data_root, args.test_set), transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "print(f\"Training on {len(train_dataset)} images, testing on {len(test_dataset)} images.\")\n",
    "\n",
    "model = CNN(input_channels = 1, img_size = args.img_size, num_classes = num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model, per_epoch_loss, per_epoch_acc, train_preds, train_targets = train(model, train_loader, optimizer, criterion, args.num_epochs, \n",
    "                                                                         args.log_interval, device )\n",
    "test_loss, test_acc, test_preds, test_targets = test(model, test_loader, device, criterion)\n",
    "\n",
    "classes_train, overall_train_mat = get_stats(train_preds, train_targets, num_classes)\n",
    "classes_test, overall_test_mat = get_stats(test_preds, test_targets, num_classes)\n",
    "\n",
    "print(f\"Training Accuracy Standard Deviation over 17 Classes: {np.std(classes_train):.5f}\")\n",
    "print(f\"Test Accuracy Standard Deviation over 17 Classes: {np.std(classes_test):.5f}\")\n",
    "\n",
    "\n",
    "print(f'\\n\\nTrain accuracy: {per_epoch_acc[-1]*100:.3f}')\n",
    "print(f'Test accuracy: {test_acc*100:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07334f-ce7e-4bcc-8167-d303341f168e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "labels = [\"CM\", \"CMM\", \"P1\", \"P2\", \"P3\", \"P31M\", \"P2M1\", \"P4\", \"P4G\", \"P4M\", \"P6\", \"P6M\", \"PG\", \"PGG\", \"PM\", \"PMG\", \"PMM\"]\n",
    "outputs = []\n",
    "for i in range(0, len(test_loader.dataset), 200):\n",
    "    layer_out = []\n",
    "    img = torch.unsqueeze(test_loader.dataset[i][0].to(device), 0)\n",
    "    for layer in list(model.children())[0]:\n",
    "        img = layer(img)\n",
    "        layer_out.append(img)\n",
    "    outputs.append(layer_out)\n",
    "fig = plt.figure(figsize = (12, 12))\n",
    "for i in range(len(outputs)):\n",
    "    img = torch.mean(torch.squeeze(outputs[i][1], 0), 0).cpu().detach().numpy()\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(img, cmap = \"gray\")\n",
    "    plt.title(labels[i])\n",
    "# plt.savefig(\"layer.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8455626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tsne(model, test_loader, args.device, \"tsne.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
