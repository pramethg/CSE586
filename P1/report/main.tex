\documentclass[12pt,twoside,a4paper]{article} 

\usepackage{color,amssymb,amsmath,mathtools} 
\usepackage{fullpage,caption, listings,clrscode,placeins}
\usepackage{amsmath,bm}
\usepackage{subfig}
 
 % Metadata info
\newcommand{\mytitle}{Project 1} 
\newcommand{\mydate}{\today} 
\newcommand{\myauthors}{Prameth Gaddale}

% Setting Hyperref parameters
\usepackage[
	bookmarks,
	bookmarksnumbered,
	pdfpagemode={UseOutlines},
	plainpages=false,
	pdfpagelabels=true,
	pdfauthor={\myauthors},
	pdftitle={\mytitle},
	pagebackref=true,
	pdftex,
	colorlinks=true,
	linkcolor=red,
	urlcolor={blue}, 
	pagebackref=true]
	{hyperref}

\title{\mytitle}
\author{\myauthors}
\date{\mydate}

% Some useful commands from CVPR
\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et~al}\onedot}
\makeatother


\pagestyle{empty}
\usepackage{fancyref,fancyhdr}
%\usepackage[hmarginratio=1:1, top=2.0cm, bottom=5.0cm, left=1cm, right=1cm]{geometry}
\setlength{\headheight}{14pt}
\setlength{\headsep}{15pt}
\setlength{\footskip}{50pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhf[HLE,HRO]{\footnotesize{\myauthors}}
\fancyhf[HLO,HRE]{\footnotesize{\mydate}}
\fancyhf[FLO,FRE]{\footnotesize{\mytitle}} 
\fancyhf[FLE,FRO]{\thepage }


\usepackage[pdftex]{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.eps}

% 
\usepackage[numbers, sort&compress]{natbib}

\usepackage[senames,dvipsnames,svgnames,table]{xcolor}

\newenvironment{tightitemize} % Defines the tightitemize environment which modifies the itemize environment to be more compact
{\vspace{-\topsep}\begin{itemize}\itemsep1pt \parskip0pt \parsep0pt}
{\end{itemize}\vspace{-\topsep}} 

% If you want to write proofs
\newtheorem{claim}{Claim}[section]
\newtheorem{lemma}{Lemma}[section]

% Some useful packages (look at booktabs for good looking tables)
\usepackage{subcaption,booktabs,placeins}

%BEGIN THE DOCUMENT

\begin{document}
\maketitle

\begin{abstract}

In this day and age, automation is crucial in various industries which involve laborious and repetitive tasks. This project covers regression and classification problems, which are considered to be one of basic techniques of \textit{Pattern Recognition}. I've implemented linear models for regression, along with methods to optimize the parameters/weights of these models through the use of \textit{Maximum Likelihood Estimation} and \textit{Maximum A Posteriori Estimation}. Additonally, for classification models I've implemented the Fisher projection algorithm for the \textit{Taiji} dataset. I've also introduced the key aspects of the \textit{Central Limit Theorem}, in addition to answering few questions regarding estimating probability distributions.

\end{abstract}
\vspace{1ex}

\tableofcontents
% Break until the next page
\vfill
\pagebreak

% SECTIONS

\section{Linear Regression}

Supervised learning problems with the training data comprising of input vectors along with their corresponding target vectors, and the desired output consisting of one or more continuous variables is called \textit{Regression}. Bishop\cite{ChrisPRML} mentions about the use of polynomial curve fitting as an example to illustrate the usage and role of linear regression in real-world problems.\\
In polynomial curve fitting, we are given a training set (of $N$ examples) of $\textbf{x}$, where $\textbf{x} \equiv (x_1,...,x_N)^T$ and their corresponding target values $\textbf{t} \equiv (t_1,...,t_N)^T$.\\
Primarily, the objective of polynomial curve fitting is to find relevant parameters(represented by polynomial coefficients) $w_0, w_1, ... w_M$ that fit a polynomial function of degree $M$ represented as \ref{POLY},

\begin{equation}\label{POLY}
    y(x, \textbf{w}) = w_0 + w_1x + w2+w^2 + ... + w_Mx^M = \sum_{j=0}^{M}w_jx^j
\end{equation}
To arrive to our objective, we have to parameters that satisfy the minimization of an \textit{error function}, that measures the magnitude of misfit between our targets and predictions. \\ 
A suitable criteria for optimizing the model parameters is to minimize the squared sum between the targets and predicted values, mentioned in \ref{ERRML1}.
\begin{equation}\label{ERRML1}
    E(\textbf{w}) = \frac{1}{2}\sum_{n=1}^{N}\{y(x^n, \textbf{w})-t_n\}^2
\end{equation}
The two most popular approaches to solve the error function and estimate the model parameters which take the uncertainty into consideration through the use of Bayesian statistics are:
\begin{enumerate}
    \item Maximum Likelihood Estimator
    \item Maximum A Posteriori Estimator
\end{enumerate}
%-------------------------------------------------------------------------
\subsection{Maximum Likelihood Estimator}
In Maximum Likelihood Estimation, the main assumption is that the ground truth values (represented by $t_i$) follow a Gaussian distribution. The assumed distribution is spread over with mean value represented by the model prediction values, and the variance matrix (for multivariate datasets) as shown in \eqref{1.60}.

\begin{equation}\label{1.60}
    p(t|x,\textbf{w}, \beta) = \mathcal{N}(t|y(x,\textbf(w)),\beta^{-1})
\end{equation}
However, for a multivariate Gaussian distribution if we apply the i.i.d criterion which states that each example sampled from the dataset is independent and identically distributed we can write the joint conditional probability distribution in the product-form as \eqref{1.61}
\begin{equation}\label{1.61}
    p(t|x,\textbf{w}, \beta) = \prod_{n=1}^{N}\mathcal{N}(t|y(x,\textbf(w)),\beta^{-1})
\end{equation}
We also know that the standard Gaussian distribution is represented by \eqref{1.46},
\begin{equation} \label{1.46}
    \mathcal{N}(x|\mu,\sigma^2) = \frac{1}{(2\pi\sigma^2)^{1/2}}\exp\{-\frac{1}{2\sigma^2}(x-\mu)^2\}
\end{equation}
Now taking the equation \eqref{1.61}, applying the general equation \eqref{1.46} and taking the natural-logarithm across both the sides, we get,
\begin{align} \label{1.62}
\ln p(t|x,\textbf{w}, \beta) =& \sum_{n=1}^{N} \ln \{ \mathcal{N}(t_n|y(x_n,\textbf{w}),\beta^{-1}) \} \\
=& \sum_{n=1}^{N} \ln \{ \frac{1}{(2\pi\beta^{-1})^{1/2}}\exp\{-\frac{1}{2\beta^{-1}}\{t_n-y(x_n,\textbf{w})\}^2\}\} \\
=&\sum_{n=1}^{N} \ln \{ \frac{1}{(2\pi\beta^{-1})^{1/2}} \} + \sum_{n=1}^{N} \ln \{\exp\{-\frac{1}{2\beta^{-1}}\{t_n-y(x_n,\textbf{w})\}^2\}\} \\
=& N\ln \{ \frac{\beta}{(2\pi)} \}^{1/2} + \sum_{n=1}^{N} {-\frac{\beta}{2}\{y(x_n,\textbf{w})-t_n\}^2} \\
\ln p(t|x,\textbf{w}, \beta) =& -\frac{\beta}{2}\sum_{n=1}^{N} {\{y(x_n,\textbf{w})-t_n\}^2} + \frac{N}{2}\ln\beta - \frac{N}{2}\ln(2\pi) \label{LOGLL}
\end{align}
From statistics, we know that \eqref{LOGLL} is represented as log-likelihood and the first term on the right-hand-side, $-\frac{\beta}{2}\sum_{n=1}^{N} {\{y(x_n,\textbf{w})-t_n\}^2}$ resembles the \textit{\textbf{sum of the squared error}}. Hence, maximizing the log-likelihood function is equivalent to minimizing the negative log-likelihood function. \\
Taking negative on both sides, we get the equation of negative log-likelihood which is used to 
\begin{equation} \label{NLL}
    -\ln p(t|x,\textbf{w}, \beta) = \frac{\beta}{2}\sum_{n=1}^{N} {\{y(x_n,\textbf{w})-t_n\}^2} - \frac{N}{2}\ln\beta + \frac{N}{2}\ln(2\pi)
\end{equation}
We can represent the error function of \textit{Maximum Likelihood Estimator} by \eqref{ERRML2}
\begin{equation}\label{ERRML2}
    E_{ML}(\textbf{w}) = \frac{\beta}{2}\sum_{n=1}^{N} {\{y(x_n,\textbf{w})-t_n\}^2} - \frac{N}{2}\ln\beta + \frac{N}{2}\ln(2\pi)
\end{equation}
One of the fundamental operation to find critical points of a function with respect to a specific parameter is to equate the partial derivative of the function to zero and solving for the parameter. Applying that strategy to equation \eqref{ERRML2} with respect to \textbf{w} we get,
\begin{align}
\frac{\partial E_{ML}(\textbf{w})}{\partial \textbf{w}} =& 0\\
\frac{\partial}{\partial \textbf{w*}} \left( \frac{\beta}{2}\sum_{n=1}^{N} {\{y(x_n,\textbf{w})-t_n\}^2} - \frac{N}{2}\ln\beta + \frac{N}{2}\ln(2\pi) \right ) =& 0 \\
\frac{1}{2}.2.\sum_{n=1}^{N}\frac{\partial y_n}{\partial \textbf{w}_i}(y_n-t_n) =& 0 \\
\sum_{n=1}^{N}\frac{\partial y_n}{\partial \textbf{w}_i}(\sum_{j=0}^{M}{w_jx^j_n}-t_n) =& 0 \\
\sum_{n=1}^{N}x_n^i(\sum_{j=0}^{M}{w_jx^j_n}-t_n) =& 0 \\
\sum_{n=1}^{N}x_n^i\sum_{j=0}^{M}{w_jx^j_n} - \sum_{n=1}^{N}x_n^it_n =& 0 \\
\sum_{n=1}^{N}\sum_{j=0}^{M}{x_n^iw_jx^j_n} - \sum_{n=1}^{N}x_n^it_n =& 0 \\
\sum_{n=1}^{N}\sum_{j=0}^{M}{x^{i+j}_nw_j} - \sum_{n=1}^{N}x_n^it_n =& 0 \label{MLDERSUM}
\end{align}
Converting the result from the summation-scalar format present in \eqref{MLDERSUM} to matrix-format we get,
\begin{align}
\textbf{X}^T\textbf{X}\textbf{w}_{ML} - \textbf{X}^Tt =& 0 \\
\textbf{X}^T\textbf{X}\textbf{w*}_{ML} =& \textbf{X}^Tt \\
\textbf{w}_{ML} =& (\textbf{X}^T\textbf{X})^{-1}\textbf{X}^Tt \label{WMLFINAL}
\end{align}
Where, \eqref{WMLFINAL} represents the optimal weights for the polynomial with respect to minimizing the negative log-likelihood.\\
We can find the predictions of the model function by applying,
\begin{equation} \label{MLPRED}
    \textbf{y} = \textbf{w}^T_{ML}\textbf{X}
\end{equation}
\subsubsection*{Design Matrix Input}
However, in real-life situations the input is in the form of multiple dimensions and is a function of the input vector either through pre-processing steps or through the use of higher dimensional polynomial for the model fit. In that case we can consider the design matrix in the form of $\phi(x_n)$ and modify our objective function as,
 \begin{equation}
     \triangledown\ln p(t|w,\beta) = \triangledown \left( \sum_{n=1}^{N}\{t_n - \textbf{w}^T\phi(x_n)\}^2 \right)
 \end{equation}
Applying partial derivative with respect to \textbf{w} and solving for \textbf{w*} we get

\begin{align}
\frac{\partial E_ML(\textbf{w})}{\partial \textbf{w*}} =& 0\\
\frac{\partial}{\partial \textbf{w*}} \{ \frac{1}{2}\sum_{n=1}^{N}\{t_n - \textbf{w}^T\phi(x_n)\}^2 \} =& 0 \\
\sum_{n=1}^{N}t_n\phi(x_n)^T - \textbf{w}^T\left(\sum_{n=1}^{N}{\phi(x_n)\phi(x_n)^T} \right) =& 0 \\
\textbf{w}^T\left(\sum_{n=1}^{N}{\phi(x_n)\phi(x_n)^T} \right) =& \sum_{n=1}^{N}t_n\phi(x_n)^T \\
\textbf{w}^T(\Phi\Phi^T) =&\textbf{t}\Phi^T \\
\textbf{w}^T =& (\Phi\Phi^T)^{-1}\textbf{t}\Phi^T \\
\textbf{w}_{ML} =& (\Phi^T\Phi)^{-1}\Phi^T\textbf{t} \label{WML}
\end{align}
Hence we can find the predictions of the model function by applying,
\begin{equation} \label{MLPRED2}
    y = \textbf{w}^T\phi(x_n)
\end{equation}
Similarly, we can find the optimal value of the parameter $\beta$ by \eqref{BETAML},
\begin{equation} \label{BETAML}
    \beta_{ML}^{-1} = \frac{1}{N}\sum_{n=1}^{N}\{t_n - \textbf{w}_{ML}^T\phi(x_n)\}^2
\end{equation}
Hence, the distribution represented by the Gaussian distribution assumption gets converted through the use of optimal values as,
\begin{equation} \label{PREDDML}
    p(t|x,\textbf{w}_{ML},\beta_{ML}) = \mathcal{N}(y(x,\textbf{w}_{ML}), \beta^{-1}_{ML})
\end{equation}

%-------------------------------------------------------------------------
\subsection{Maximum A Posteriori Estimator}
Unlike \textit{Maximum Likelihood Estimation} where the posterior distribution on the ground truth was assumed to be from a Gaussian distribution, Maximum A Posteriori Estimator introduces a prior distribution on the weight vector \textbf{w}. That could be written mathematically as in \eqref{LN1}. Using the general Gaussian distribution equation and applying natural logarithm to \eqref{LNFINAL}.
\begin{align} 
p(\textbf{w}|\alpha) =& \mathcal{N}(\textbf(w)|0,\alpha^{-1}\textbf{I}) \label{LN1}\\
=& \frac{1}{(2\pi\alpha^{-1})^{1/2}}\exp\{-\frac{1}{2\alpha^{-1}}(\textbf{w}-0)^2\} \\
p(\textbf{w}|\alpha) =& \left( \frac{\alpha}{2\pi}\right)^{(M+1)/2} \exp\left\{ -\frac{\alpha}{2}\textbf{w}^T\textbf{w}\right\} \\
\ln p(\textbf{w}|\alpha) =& \frac{(M+1)}{2}.\ln\left( \frac{\alpha}{2\pi}\right) - \left\{ \frac{\alpha}{2}\textbf{w}^T\textbf{w}\right\} \label{LNFINAL}
\end{align}
From probability theory we know that,
\begin{align}
Posterior \propto& \ Likelihood * Prior\\
    p(\textbf{w}|\textbf{x},\textbf{t},\alpha,\beta) \propto& p(\textbf{t}|\textbf{x},\textbf{w},\beta) p(\textbf{w}|\alpha) \\
    \ln p(\textbf{w}|\textbf{x},\textbf{t},\alpha,\beta) \propto& \ln p(\textbf{t}|\textbf{x},\textbf{w},\beta) + \ln p(\textbf{w}|\alpha) \\
    \ln p(\textbf{w}|\textbf{x},\textbf{t},\alpha,\beta) =& \ln p(\textbf{t}|\textbf{x},\textbf{w},\beta) + \ln p(\textbf{w}|\alpha) + constant \label{PLP}
\end{align}
Substituting the equations \eqref{LOGLL} and \eqref{LNFINAL} in \eqref{PLP} we get,

\begin{multline}
        \ln p(\textbf{w}|\textbf{x},\textbf{t},\alpha,\beta) = {\left( -\frac{\beta}{2}\sum_{n=1}^{N} {\{y(x_n,\textbf{w})-t_n\}^2} + \frac{N}{2}\ln\beta - \frac{N}{2}\ln(2\pi) \right )} \\ + \left ( \frac{(M+1)}{2}.\ln\left( \frac{\alpha}{2\pi}\right) - \left\{ \frac{\alpha}{2}\textbf{w}^T\textbf{w}\right\} \right ) + constant
\end{multline}

\begin{multline}
        \ln p(\textbf{w}|\textbf{x},\textbf{t},\alpha,\beta) = -\frac{\beta}{2}\sum_{n=1}^{N} {\{y(x_n,\textbf{w})-t_n\}^2} + \frac{N}{2}\ln\beta - \frac{N}{2}\ln(2\pi) \\ + \frac{(M+1)}{2}.\ln\left( \frac{\alpha}{2\pi}\right) - \left\{ \frac{\alpha}{2}\textbf{w}^T\textbf{w}\right\} + constant
\end{multline}

\begin{multline} \label{NLLMAP}
       - \ln p(\textbf{w}|\textbf{x},\textbf{t},\alpha,\beta) = \frac{\beta}{2}\sum_{n=1}^{N} {\{y(x_n,\textbf{w})-t_n\}^2} - \frac{N}{2}\ln\beta + \frac{N}{2}\ln(2\pi) \\ - \frac{(M+1)}{2}.\ln\left( \frac{\alpha}{2\pi}\right) + \left\{ \frac{\alpha}{2}\textbf{w}^T\textbf{w}\right\} + constant
\end{multline}

Ignoring the constants (for simplicity as they would be equate to zero on taking the partial derivative with respect to \textbf{w}) from \eqref{NLLMAP} we can find the maximum of the posterior by minimizing the following equation,
\begin{equation}
     - \ln p(\textbf{w}|\textbf{x},\textbf{t},\alpha,\beta) = \frac{\beta}{2}\sum_{n=1}^{N} {\{y(x_n,\textbf{w})-t_n\}^2} + \left\{ \frac{\alpha}{2}\textbf{w}^T\textbf{w}\right\} 
\end{equation}

\begin{align}
    \triangledown \left(- \ln p(\textbf{w}|\textbf{x},\textbf{t},\alpha,\beta) \right ) =& 0 \\
    \triangledown \left( \frac{\beta}{2}\sum_{n=1}^{N} {\{y(x_n,\textbf{w})-t_n\}^2} + \left\{ \frac{\alpha}{2}\textbf{w}^T\textbf{w}\right\}  \right ) =& 0 \\
    \triangledown \left( \frac{\beta}{2}\sum_{n=1}^{N} {\{ \textbf{w}^T\textbf{X}-t_n\}^2} + \left\{ \frac{\alpha}{2}\textbf{w}^T\textbf{w}\right\}  \right ) =& 0 \\
 \textbf{w} =& \left(\textbf{X}^T\textbf{X} + \frac{\alpha}{\beta}\textbf{I}\right)^{-1}\textbf{X}^T\textbf{t} \\
 \textbf{w}_{MAP} =& \left(\textbf{X}^T\textbf{X} + \lambda\textbf{I}\right)^{-1}\textbf{X}^T\textbf{t}
\end{align}

We can observe that, this optimization criterion is equivalent to minimizing the regularized sum-of-squares error, with the parameter $\lambda = \alpha/\beta$.

%-------------------------------------------------------------------------
\subsection{Report}
\subsubsection{Dataset}
Given dataset consists of input values($x$), ground truth values($y$), target values($t$) and random noise values($sigma$) added to ground truth values to produce targets.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.7\linewidth]{results/gt_data.png}
    \caption{Given training dataset \textit{data.npz}. Consists of ($x$), ($y$), ($t$) and($sigma$) values.}
    \label{fig: DATASET}
\end{figure}
%-------------------------------------------------------------------------

\pagebreak
\subsubsection{Visualization}

\subsubsection*{Maximum Likelihood Estimation}

\begin{figure}[!htb]
    \centering
    \subfloat[Model Degree: $0$]{
      \includegraphics[width=0.55\linewidth]{results/50_ml_0.png}}
    \subfloat[Model Degree: $1$]{
      \includegraphics[width=0.55\linewidth]{results/50_ml_1.png}}
    \hspace{0mm}
    \subfloat[Model Degree: $3$]{
      \includegraphics[width=0.55\linewidth]{results/50_ml_3.png}}
    \subfloat[Model Degree: $9$]{
      \includegraphics[width=0.55\linewidth]{results/50_ml_9.png}}
    \hspace{0mm}
    \caption{Maximum Likelihood Estimation Model Prediction Visualization}
\end{figure}

\pagebreak
\subsubsection*{Maximum A Posteriori Estimation}

\begin{figure}[!htb]
    \centering
    \subfloat[Model Degree: $0$]{
      \includegraphics[width=0.55\linewidth]{results/50_map_0.png}}
    \subfloat[Model Degree: $1$]{
      \includegraphics[width=0.55\linewidth]{results/50_map_1.png}}
    \hspace{0mm}
    \subfloat[Model Degree: $3$]{
      \includegraphics[width=0.55\linewidth]{results/50_map_3.png}}
    \subfloat[Model Degree: $9$]{
      \includegraphics[width=0.55\linewidth]{results/50_map_9.png}}
    \hspace{0mm}
    \caption{Maximum A Posteriori Estimation Model Prediction Visualization}
\end{figure}

%-------------------------------------------------------------------------
\subsubsection{Comparison}

%-------------------------------------------------------------------------
\subsubsection{Summary}

%-------------------------------------------------------------------------
\pagebreak
\subsection{Extra Credit}
\subsubsection*{Here are my solutions for the extra credit problems.}

%-------------------------------------------------------------------------
\subsubsection{Additional Lambda Values}

%-------------------------------------------------------------------------
\subsubsection{Varying Degree of Polynomial Order}

\begin{table}[!htb]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Weights}              & \textbf{M = 0} & \textbf{M = 1} & \textbf{M = 3} & \textbf{M = 6} & \textbf{M = 9} \\\hline
\textit{\textbf{$w_0^{ML}$}} & 0.009549       & 1.204800       & -0.185197      & -0.713196      & -1.367908      \\\hline
\textit{\textbf{$w_1^{ML}$}}  & -              & -0.176208      & 0.958219       & 1.860958       & 4.790468       \\\hline
\textit{\textbf{$w_2^{ML}$}} & - & - & -0.221176 & -0.794732 & -4.971345 \\\hline
\textit{\textbf{$w_3^{ML}$}} & - & - & 0.011743  & 0.184684  & 2.956968  \\\hline
\textit{\textbf{$w_4^{ML}$}} & - & - & -         & -0.026116 & -1.027571 \\\hline
\textit{\textbf{$w_5^{ML}$}} & - & - & -         & 0.001895  & 0.214036  \\\hline
\textit{\textbf{$w_6^{ML}$}} & - & - & -         & -0.000052 & -0.027113 \\\hline
\textit{\textbf{$w_7^{ML}$}} & - & - & -         & -         & 0.002043  \\\hline
\textit{\textbf{$w_8^{ML}$}} & - & - & -         & -         & -0.000084 \\\hline
\textit{\textbf{$w_9^{ML}$}} & - & - & -         & -         & 0.000001 \\\hline
\end{tabular}
\caption{Weights of the Maximum Likelihood model with varying degrees.}
\label{MLWEIGHTS}
\end{table}

\begin{table}[!htb]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Weights}              & \textbf{M = 0} & \textbf{M = 1} & \textbf{M = 3} & \textbf{M = 6} & \textbf{M = 9} \\ \hline
\textit{\textbf{$w_0^{MAP}$}} & 0.009549       & 1.204745       & -0.184890      & -0.674439      & 0.263706       \\ \hline
\textit{\textbf{$w_1^{MAP}$}} & -              & -0.176201      & 0.958032       & 1.803124       & 0.875506       \\ \hline
\textit{\textbf{$w_2^{MAP}$}} & - & - & -0.221146 & -0.764588 & -1.278764 \\ \hline
\textit{\textbf{$w_3^{MAP}$}} & - & - & 0.011742  & 0.177347  & 1.131148  \\ \hline
\textit{\textbf{$w_4^{MAP}$}} & - & - & -         & -0.025209 & -0.497739 \\ \hline
\textit{\textbf{$w_5^{MAP}$}} & - & - & -         & 0.001839  & 0.119170  \\ \hline
\textit{\textbf{$w_6^{MAP}$}} & - & - & -         & -0.000051 & -0.016513 \\ \hline
\textit{\textbf{$w_7^{MAP}$}} & - & - & -         & -         & 0.001324  \\ \hline
\textit{\textbf{$w_8^{MAP}$}} & - & - & -         & -         & -0.000057 \\ \hline
\textit{\textbf{$w_9^{MAP}$}} & - & - & -         & -         & 0.000001  \\ \hline
\end{tabular}
\caption{Weights of the Maximum A Posteriori model with varying degrees.}
\label{MAPWEIGHTS}
\end{table}


%-------------------------------------------------------------------------
\subsubsection{Varying Number of Sample Points}

%-------------------------------------------------------------------------
\subsubsection{Curse of Dimensionality}

%-------------------------------------------------------------------------
\pagebreak
\section{Classification}
Unlike regression problems, the objective of classification models is to assign input vector $\textbf{x}$ to one of \textit{K} discrete classes where $k = 1,...,K$. The \textit{decision regions} or \textit{decision boundaries} therefore represent the division of the input space, which are used to classify each input into disjoint classes.\\
\textit{Fisher Linear Discriminant Analysis} is considered to be a part of the family of linear models used for classification, where the decision boundaries are linear functions of the provided input vector $\textbf{x}$.
%-------------------------------------------------------------------------
\subsection{Fisher Projection}
In \textit{Fisher Linear Discriminant}, the model tries to reduce the dimension of the
\begin{equation}
    \textbf{y} = \textbf{W}^T\textbf{x}
\end{equation}

\begin{equation}
    \textbf{S}_W = \sum_{k=1}^K\textbf{S}_k
\end{equation}
where the term $\textbf{S}_k$ is represented by,
\begin{equation}
    \textbf{S}_k = \sum_{n\in\textit{C}_k}(x_n - \textbf{m}_k)(x_n - \textbf{m}_k)^T
\end{equation}

\begin{equation}
    \textbf{m}_1 = \frac{1}{N_1}\sum_{n\in\textit{C}_1}x_n,\qquad \textbf{m}_2 = \frac{1}{N_2}\sum_{n\in\textit{C}_2}x_n
\end{equation}

\begin{equation}
    \textbf{m}_k = \frac{1}{N_k}\sum_{n\in\textit{C}_k}\textbf{x}_n
\end{equation}

\begin{equation}
    \textbf{S}_T = \sum_{n=1}^N(\textbf{x}_n - \textbf{m})(\textbf{x}_n - \textbf{m})^T
\end{equation}

\begin{equation}
    \textbf{S}_B = \sum_{n=1}^N N_K(\textbf{m}_k - \textbf{m})(\textbf{m}_k - \textbf{m})^T
\end{equation}

\begin{equation}
    \textbf{m} = \frac{1}{N}\sum_{k=1}^K N_k\textbf{m}_k
\end{equation}

\begin{align}
    J(\textbf{W}) =& Tr\left\{ s_W^{-1}s_B \right \} \\
    J(\textbf{W}) =& Tr\left( (\textbf{W}\textbf{S}_W\textbf{W}^T)^{-1} (\textbf{W}\textbf{S}_B\textbf{W}^T) \right )
\end{align}

%-------------------------------------------------------------------------
\subsection{Report}

%-------------------------------------------------------------------------
\subsubsection{Confusion Matrix}

%-------------------------------------------------------------------------
\subsubsection{Classification Rates}

%-------------------------------------------------------------------------
\subsubsection{Analysis}

%-------------------------------------------------------------------------
\subsubsection{Conclusion}

%-------------------------------------------------------------------------
\subsection{Extra Credit}

%-------------------------------------------------------------------------
\pagebreak
\section{Central Limit Theorem}

%-------------------------------------------------------------------------
\subsection{Introduction}
According to Fischer \cite{CLT}, classical central limit theorem states that for a large enough sample size the population distribution can be assumed to be Gaussian. However, its essential to note that the sequence of random variables forming the population must be drawn from a distribution with finite mean and variance and must follow the i.i.d. criterion.

Let us consider an unknown distribution with $n$ independent, identically distributed random variables $X_1, X_2, ..., X_n$, where is $n$ is very large with mean, $\mu$ and variance, $\sigma^2$. \\

Hence, we can find the mean, $\mu_N$ and variance, $\sigma^2_N$ as,
\begin{align}
\mu_N =& \frac{1}{n}(X_1+X_2+...+X_n) \\
      =& \frac{1}{n}(n*\mu) \\
\mu_N =& \mu
\end{align}

\begin{align}
\sigma^2_N =& Var\left( \frac{1}{n}(X_1+X_2+...+X_n) \right ) \\
           =& \left( \frac{1}{n} \right )^2 \left(Var(X_1)+Var(X_2)+...+Var(X_n) \right ) \\
           =& \left( \frac{1}{n} \right )^2 \left(\sigma^2+\sigma^2+...+\sigma^2 \right ) \\
           =& \left( \frac{1}{n} \right )^2(n\sigma^2) \\
\sigma^2_N =& \frac{\sigma^2}{n}
\end{align}

Hence, the approximate Gaussian distribution is formed with a mean, $\mu_N$ and variance, $\sigma^2_N$.
%-------------------------------------------------------------------------
\subsection{Question 1}

\subsubsection*{Question:}
\textbf{For a coin with a probability of heads of 0.6 in each flip, calculate the exact distribution of the total number of heads when the coin is flipped 5 times. Provide the equation(s) used to generate the result and a brief explanation of you reasoning.}

\subsubsection*{Solution:}
\label{Q1 Solution}
When a coin is flipped, there are two possible outcomes: head and tails each carrying a fixed probability. Given is coin is imbalanced in nature with the probability of head in each flip being \textbf{$0.6$}.\\
Binary outcome experiments such as coin-toss follow the \textbf{Binomial Distribution}. Let $X$ be the binomial random variable over the coin-toss experiment.\\
Hence, the probability mass function for the experiment representing the binomial random variable $X$ is given by,
\begin{equation} \label{BD1}
F(\mathbf{X = x}) = 
    \begin{cases}
        {{n}\choose{x}}p^{x}q^{n-x} & ,x = 0,1,2,...,n\\
        0   & ,otherwise
    \end{cases}
\end{equation}
The parameters in the binomial distribution are:
\begin{itemize}
    \item $n$: The total number of trials.
    \item $p$: The probability of success on a single trial.
    \item $q$: The probability of failure on a single trial.
    \item $x$: Represents the trial number(which is a whole number, $0\le x \le n$)
\end{itemize}
In our case, the probability of heads and tails adds up to 1. Hence we can write,
$$q=1-p$$
By incorporating that change in \eqref{BD1} we get,
\begin{equation} \label{BD2}
F(\mathbf{X = x}) = 
    \begin{cases}
        {{n}\choose{x}}p^{x}(1-p)^{n-x} & ,x = 0,1,2,...,n\\
        0   & ,otherwise
    \end{cases}
\end{equation}
We get $n=5$(as total number of coin flips is 5), $p=0.6$ and $q=0.4$.
\begin{equation} \label{BD3}
F(\mathbf{X = x}) = 
    \begin{cases}
        {{5}\choose{x}}(0.6)^{x}(0.4)^{5-x} & ,x = 0,1,2,...,n\\
        0   & ,otherwise
    \end{cases}
\end{equation}
Therefore \eqref{BD3} represents the probability distribution of total number of heads when the coin is flipped 5 times.\\
In short hand notation, we can also write that,
$$S_n \sim Binomial(n=5, p=0.6)$$
Hence, we can say that the exact distribution of the total number of heads if coin is flipped 5 times is represented by a Binomial Distribution with parameters $n=5$ and $p=0.6$.
%-------------------------------------------------------------------------
\subsection{Question 2}

\subsubsection*{Question:}
\textbf{Using the central limit theorem, approximate the distribution of the total number of heads when the coin is flipped 5000 times. Provide the equation(s) used to generate the result and a brief explanation of you reasoning.}

\subsubsection*{Solution:}
The coin-toss experiment as we know from \ref{Q1 Solution}, follows the Binomial Distribution for the probability estimation. However, for 5000 coin flips, calculation of probability through the use of factorials in the combination terms becomes a computationally expensive. From central limit theorem we know that, for a large number of trails in an experiment, we can approximate the population to be of a normal distribution. \\
\begin{itemize}
    \item Let $F_n$ represent the probability distribution we need to estimate for 5000 coin flips, which comes from a \textbf{Binomial Distribution}. $$F_n \sim Binomial(n, p)$$
    \item Let the approximate normal distribution have a mean, $\mu_N$ and variance $\sigma^2_N$.
    \item Let the approximate normal distribution have a mean, $\mu_N$ and variance $\sigma^2_N$.
\end{itemize}
From central limit theorem, we know that,
\begin{equation}\label{DMEAN}
    \mu_N = \mu
\end{equation}
\begin{equation}\label{DVAR}
    \sigma_N^2 = \frac{\sigma^2}{n}
\end{equation}

Some useful properties are:
\begin{itemize}
    \item \textbf{Mean}: $\mu = np$
    \item \textbf{Variance}: $\sigma^2 = np(1-p)$
\end{itemize}
For our case, we get,
$$F_{n = 5000} \sim Binomial(n = 5000, p = 0.6)$$
\begin{align*}
    Mean, \mu =& n*p \\
        =& 5000*0.6 \\
        \mu=& 3000
\end{align*}
\begin{align*}
    Variance, \sigma^2 =& n*p*(1-p) \\
        =& 5000*0.6*0.4 \\
        \mu=& 1200
\end{align*}
From \eqref{DMEAN} and \eqref{DVAR}, we get
$$\mu_N = 3000$$
\begin{align*}
    \sigma_N^2 =& \frac{\sigma^2}{n} \\
               =& \frac{1200}{5000}  \\
    \sigma_N^2 =& 0.24
\end{align*}
$$F_{n=5000} \sim Normal(\mu_N = 3000, \sigma^2_N = 0.24)$$
Hence, we can say that the exact distribution of the total number of heads if coin is flipped 5000 times can be approximated using Central Limit Theorem by a Gaussian Distribution with parameters $\mu_N = 3000$ and $\sigma^2_N = 0.24$.
%-------------------------------------------------------------------------

% BIBLIOGRAPHY
{\small
\bibliographystyle{IEEEtranN}
\bibliography{mybib}}

\end{document}
