\documentclass[12pt,twoside,a4paper]{article} 

\usepackage{color,amssymb,amsmath,mathtools} 
\usepackage{fullpage,caption, listings,clrscode,placeins}
\usepackage{amsmath,bm}
\usepackage{subfig}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
% \usepackage{graphicx}
 
 % Metadata info
\newcommand{\mytitle}{Project 2} 
\newcommand{\mydate}{\today}
\newcommand{\myauthors}{Prameth Gaddale (pqg5273@psu.edu)}

% Setting Hyperref parameters
\usepackage[
	bookmarks,
	bookmarksnumbered,
	pdfpagemode={UseOutlines},
	plainpages=false,
	pdfpagelabels=true,
	pdfauthor={\myauthors},
	pdftitle={\mytitle},
	pagebackref=true,
	pdftex,
	colorlinks=true,
	linkcolor=red,
	urlcolor={blue}, 
	pagebackref=true]
	{hyperref}

\title{\mytitle}
\author{\myauthors}
\date{\mydate}

% Some useful commands from CVPR
\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et~al}\onedot}
\makeatother


\pagestyle{empty}
\usepackage{fancyref,fancyhdr}
%\usepackage[hmarginratio=1:1, top=2.0cm, bottom=5.0cm, left=1cm, right=1cm]{geometry}
\setlength{\headheight}{14pt}
\setlength{\headsep}{15pt}
\setlength{\footskip}{50pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhf[HLE,HRO]{\footnotesize{\myauthors}}
\fancyhf[HLO,HRE]{\footnotesize{\mydate}}
\fancyhf[FLO,FRE]{\footnotesize{\mytitle}} 
\fancyhf[FLE,FRO]{\thepage }


\usepackage[pdftex]{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.eps}

% 
\usepackage[numbers, sort&compress]{natbib}

\usepackage[senames,dvipsnames,svgnames,table]{xcolor}

\newenvironment{tightitemize} % Defines the tightitemize environment which modifies the itemize environment to be more compact
{\vspace{-\topsep}\begin{itemize}\itemsep1pt \parskip0pt \parsep0pt}
{\end{itemize}\vspace{-\topsep}} 

% If you want to write proofs
\newtheorem{claim}{Claim}[section]
\newtheorem{lemma}{Lemma}[section]

% Some useful packages (look at booktabs for good looking tables)
\usepackage{subcaption,booktabs,placeins}

%BEGIN THE DOCUMENT

\begin{document}
\maketitle

\begin{abstract}

This project primarily deals with feature subset selection for the classification problem through the use of the Taiji sequence dataset. The use of feature selection is to solve the issues of 'Curse of Dimensionality', computational efficiency, easier data collection, storage size, and interpretability through the strategy of dimensionality reduction. Implementations of the feature selection algorithms in this project include \textbf{Filtering} and \textbf{Wrapper} methods. 

\end{abstract}
\vspace{1ex}

\tableofcontents

% SECTIONS

\section{Introduction}
The last project followed an approached where vanilla classification problem was implemented through the use of \textit{Fisher Linear Discriminant Analysis}, which would not necessarily correspond to complexity involved by higher number of feature dimensions. The approach previously taken in the \textit{Project-1} didn't involve any data normalization or feature engineering steps which are essential pre-processing steps for the data to be involved with before being used for making crucial classification decisions/predictions.

\textit{Feature Engineering}, or simply \textit{Feature Selection} are steps followed before fitting the training the final regression or classification model to improve the performance through the use of relevant features. Modern machine learning datasets contain thousands of features corresponding to the dataset used. However, each of them correspond to either useful/useless category of features used for training the parameters of the machine learning model. For example, in the given dataset there are features that don't have any variance, or have a constant value for all the training examples. It would be wise to exclude the feature in that case to gain more leverage in overall computational efficiency, storage and algorithmic efficiency.
%-------------------------------------------------------------------------
\section{Approach}

\subsection{Data}


%-------------------------------------------------------------------------
\subsection{Filter Method}

\begin{algorithm}
\caption{Filter Method Pseudocode}\label{alg:fmp}
\begin{algorithmic}

\Require $n \geq 0$

\vspace{-20pt}
\end{algorithmic}
\end{algorithm}

%-------------------------------------------------------------------------
\subsection{Wrapper Method}

%-------------------------------------------------------------------------
\section{Report}

%-------------------------------------------------------------------------
\subsection{Flowchart of the Model Pipeline}

%-------------------------------------------------------------------------
\subsection{Histograms}

%-------------------------------------------------------------------------
\subsubsection{Most Discriminative Features}

%-------------------------------------------------------------------------
\subsubsection{Most Commonly Selected Features}

%-------------------------------------------------------------------------
\subsection{Classification Results}

%-------------------------------------------------------------------------
\subsubsection{Baseline Model (No Filter, No Wrapper)}
As a baseline measure for comparison purposes, a model without feature selection of filtering and wrapping methodology was implemented. The data used for the classification involved the use of all the $1961$ features for train and test phases. The classification model used for training and inference was \textit{K-Nearest Neighbors Classifier} from \textit{Scikit-Learn} with $k=10$.

\subsubsection*{Classification Rates}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{nfnw/overall_per_class_train.png}
    \caption{Train}
    \label{fig: NFW1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{nfnw/overall_per_class_test.png}
    \caption{Test}
    \label{fig: NFW2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{nfnw/subject_wise_acc.png}
    \caption{Comparison}
    \label{fig: NFW3}
\end{figure}

\subsubsection*{Confusion Matrix}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{nfnw/overall_train_conf_mat.png}
    \caption{Train}
    \label{fig: NFW4}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{nfnw/overall_test_conf_mat.png}
    \caption{Test}
    \label{fig: NFW5}
\end{figure}

%-------------------------------------------------------------------------
\subsubsection{Filter, No Wrapper}

The filtering method with Variance Ratio was implemented and the top 100 features were considered. The processed training data was used to fit the \textit{K-Nearest Neighbors Classifier} from \textit{Scikit-Learn} with $k=10$.

\subsubsection*{Classification Rates}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{fnw/overall_per_class_train.png}
    \caption{Train}
    \label{fig: F1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{fnw/overall_per_class_test.png}
    \caption{Test}
    \label{fig: F2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{fnw/subject_wise_acc.png}
    \caption{Comparison}
    \label{fig: F3}
\end{figure}

\subsubsection*{Confusion Matrix}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fnw/overall_train_conf_mat.png}
    \caption{Train}
    \label{fig: F4}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fnw/overall_test_conf_mat.png}
    \caption{Test}
    \label{fig: F5}
\end{figure}

%-------------------------------------------------------------------------
\subsubsection{Filter, Wrapper}

The filtering method with Variance Ratio as the measure was implemented. The top 100 features passed through the wrapper method with Sequential Forward Selection. The processed training data was used to fit the \textit{K-Nearest Neighbors Classifier} from \textit{Scikit-Learn} with $k=10$.

\subsubsection*{Classification Rates}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{results/overall_per_class_train.png}
    \caption{Train}
    \label{fig: FW1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{results/overall_per_class_test.png}
    \caption{Test}
    \label{fig: FW2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{results/subject_wise_acc.png}
    \caption{Comparison}
    \label{fig: FW3}
\end{figure}

\subsubsection*{Confusion Matrix}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{results/overall_train_conf_mat.png}
    \caption{Train}
    \label{fig: FW4}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{results/overall_test_conf_mat.png}
    \caption{Test}
    \label{fig: FW5}
\end{figure}

\subsubsection*{Most Discriminative Features: Filter}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{results/selected_feats.png}
    \caption{Filter}
    \label{fig: FW6}
\end{figure}

\subsubsection*{Most Commonly Selected Features: Wrapper}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{results/avg_filter_scores.png}
    \caption{Filter}
    \label{fig: FW7}
\end{figure}

%-------------------------------------------------------------------------
\subsection{Question: Dataset Size Sufficiency}

%-------------------------------------------------------------------------
\section{Conclusion}

%-------------------------------------------------------------------------
\section{Extra Credit}
Here are my solutions for the \textbf{Extra Credit} section.
\subsection{Ensemble Learning}
%-------------------------------------------------------------------------

% BIBLIOGRAPHY
{\small
\bibliographystyle{IEEEtranN}
\bibliography{mybib}}

\end{document}
