{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e063267-844f-4f80-986c-ff7cd0205ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60a12e-81ab-4c6a-a3fa-4f3cada23e79",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829877c1-9e9c-4da6-a62f-3541a1dd83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNoisyData(num_points=50):\n",
    "    \"\"\"\n",
    "    Generates noisy sample points and saves the data. The function will save the data as a npz file.\n",
    "    Args:\n",
    "        num_points: number of sample points to generate.\n",
    "    \"\"\"\n",
    "    x = np.linspace(1, 4*math.pi, num_points)\n",
    "    y = np.sin(x*0.5)\n",
    "\n",
    "    # Define the noise model\n",
    "    nmu = 0\n",
    "    sigma = 0.3\n",
    "    noise = nmu + sigma * np.random.randn(num_points)\n",
    "    t = y + noise\n",
    "\n",
    "    # Save the data\n",
    "    np.savez('data.npz', x=x, y=y, t=t, sigma=sigma)\n",
    "\n",
    "# Feel free to change aspects of this function to suit your needs.\n",
    "# Such as the title, labels, colors, etc.\n",
    "def plot_with_shadded_bar(x=None, y=None, sigma=None):\n",
    "    \"\"\"\n",
    "    Plots the GT data for visualization.\n",
    "    Args:\n",
    "        x: x values\n",
    "        y: y values\n",
    "        sigma: standard deviation\n",
    "    \"\"\"\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "\n",
    "    # Example plotting with the GT data, you can use this as a reference. You will later \n",
    "    # use this function to plot the results of your model.\n",
    "    np.load('data.npz')\n",
    "    x = np.load('data.npz')['x']\n",
    "    y = np.load('data.npz')['y']\n",
    "    t = np.load('data.npz')['t']\n",
    "    sigma = np.load('data.npz')['sigma']\n",
    "\n",
    "    # Plot the ground truth curve of the generated data.\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot curve with red shaded region spans on std.\n",
    "    ax.plot(x, y, 'r', label='Ground Truth Example')\n",
    "    ax.fill_between(x, y-sigma, y+sigma, color='r', alpha=0.2)\n",
    "\n",
    "    # Plot the noisy data points.\n",
    "    ax.scatter(x, t, label='Noisy Data')\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('t')\n",
    "    ax.set_title('Data Point Plot Example')\n",
    "\n",
    "    plt.savefig('results/gt_data.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_results(x=None, y=None, pred1 = None, label1 = None, pred2 = None, label2 = None, title = None, file_name = None):\n",
    "    \"\"\"\n",
    "    Plots the GT data for visualization.\n",
    "    Args:\n",
    "        x: x values\n",
    "        y: y values\n",
    "        pred: prediction values\n",
    "    \"\"\"\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "\n",
    "    np.load('data.npz')\n",
    "    x = np.load('data.npz')['x']\n",
    "    y = np.load('data.npz')['y']\n",
    "    t = np.load('data.npz')['t']\n",
    "\n",
    "    # Plot the ground truth curve of the generated data.\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(x, y, 'g', label = \"Ground Truth\")\n",
    "    ax.plot(x, pred1, 'r', label = label1)\n",
    "    if pred2 is not None:\n",
    "        ax.plot(x, pred2, \"y\", label = label2)\n",
    "\n",
    "    # Plot the noisy data points.\n",
    "    ax.scatter(x, t, label='Noisy Data')\n",
    "\n",
    "    ax.set_xlabel('X Values')\n",
    "    ax.set_ylabel('Y Values, Predictions, Noisy Data')\n",
    "    ax.set_title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'results/{file_name}.png')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a518cb5-caed-45c8-9fab-01bdd7a98135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML():\n",
    "    def __init__(self, degree = None):\n",
    "        \"\"\"\n",
    "        Initializes the Maximum Likelihood model. You may add any parameters/variables you need.\n",
    "        You may also add any functions you may need to the class.\n",
    "        \"\"\"\n",
    "        self.degree = degree\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Fits the Maximum Likelihood model to the data.\n",
    "        Args:\n",
    "            x (np.array): The features of the data.\n",
    "            y (np.array): The targets of the data.\n",
    "        \"\"\"\n",
    "        x = PolynomialFeatures(degree = self.degree).fit_transform(x.reshape(-1,1))\n",
    "        intermediate = (x.T)@x\n",
    "        weights = np.linalg.solve(intermediate, (x.T)@y)\n",
    "        return weights\n",
    "\n",
    "    def predict(self, x, weights):\n",
    "        \"\"\"\n",
    "        Predicts the targets of the data.\n",
    "        Args:\n",
    "            x (np.array)      : The features of the data.\n",
    "            weights (np.array): Weights from the ML.fit() method.\n",
    "        Returns:\n",
    "            np.array: The predicted targets of the data.\n",
    "        \"\"\"\n",
    "        x = PolynomialFeatures(degree = self.degree).fit_transform(x.reshape(-1,1))\n",
    "        predictions = np.dot(x, weights)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b044b57-31ab-428f-9a11-3e2d6d66a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAP():\n",
    "    def __init__(self, alpha=0.005, beta=11.1, lnlambda = None, customReguralization = False, degree = None):\n",
    "        \"\"\"\n",
    "        Initializes the Maximum A Posteriori model. You may add any parameters/variables you need.\n",
    "        Args:\n",
    "            alpha (float): The alpha parameter of the model.\n",
    "            beta (float): The beta parameter of the model.\n",
    "\n",
    "        You may also add any functions you may need to the class.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.lnlambda = lnlambda\n",
    "        self.customReguralization = customReguralization\n",
    "        self.degree = degree\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Fits the Maximum A Posteriori model to the data.\n",
    "        Args:\n",
    "            x (np.array): The features of the data.\n",
    "            y (np.array): The targets of the data.\n",
    "        \"\"\"\n",
    "        if self.customReguralization:\n",
    "            lnlambda = self.lnlambda\n",
    "        else:\n",
    "            lnlambda = np.log(self.alpha/self.beta)\n",
    "\n",
    "        x = PolynomialFeatures(degree = self.degree).fit_transform(x.reshape(-1,1))\n",
    "        intermediate = ((x.T)@x) + np.exp(lnlambda)*np.eye(x.shape[1], x.shape[1])\n",
    "        weights = np.linalg.solve(intermediate, (x.T)@y)\n",
    "        return weights\n",
    "\n",
    "    def predict(self, x, weights):\n",
    "        \"\"\"\n",
    "        Predicts the targets of the data.\n",
    "        Args:\n",
    "            x (np.array): The features of the data.\n",
    "            weights (np.array): The weights from the MAP.fit() method.\n",
    "        Returns:\n",
    "            np.array: The predicted targets of the data.\n",
    "        \"\"\"\n",
    "        x = PolynomialFeatures(degree = self.degree).fit_transform(x.reshape(-1,1))\n",
    "        predictions = np.dot(x, weights)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8921606-28b6-43eb-b4e9-848a613a8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNoisyData(50)\n",
    "plot_with_shadded_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca81d68-6272-4926-aab0-fb6b75abca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('data.npz')\n",
    "x = np.load('data.npz')['x']\n",
    "y = np.load('data.npz')['y']\n",
    "t = np.load('data.npz')['t']\n",
    "sigma = np.load('data.npz')['sigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a86f1d-c715-4d02-b74e-ea4b04c9b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_Model = ML(degree = 3)\n",
    "ml_weights = ML_Model.fit(x, t)\n",
    "ml_predictions = ML_Model.predict(x, ml_weights)\n",
    "plot_results(x, y, ml_predictions, title = \"ML Model Degree 3\", file_name = \"ml_3\", label1 = \"ML Model Predictions\")\n",
    "\n",
    "MAP_Model = MAP(degree = 3, customReguralization = False)\n",
    "map_weights = MAP_Model.fit(x, t)\n",
    "map_predictions = MAP_Model.predict(x, map_weights)\n",
    "plot_results(x, y, map_predictions, title = \"MAP Model Degree 3\", file_name = \"map_3\", label1 = \"MAP Model Predictions\")\n",
    "\n",
    "plot_results(x, y, pred1 = ml_predictions, label1 = \"ML Model Degree 3\", pred2 = map_predictions, label2 = \"MAP Model Degree 3\", title = \"ML vs MAP Predictions\", file_name = \"mlevsmap\")\n",
    "\n",
    "lnlambda = -18\n",
    "CustomModel = MAP(degree = 3, customReguralization = True, lnlambda = lnlambda)\n",
    "custom_weights = CustomModel.fit(x, t)\n",
    "custom_predictions = CustomModel.predict(x, custom_weights)\n",
    "plot_results(x, y, custom_predictions, title = r\"Custom Model Degree 3, ln$\\lambda$ = \" + str(lnlambda), file_name = \"lnlambda-18\", label1 = r\"$ln\\lambda$ = \"+str(lnlambda)+\" Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f64b7-07dc-420f-89be-b8de236c8189",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2cb49-70d3-4223-810f-bfa6945e20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_desc_bounds(classifier, feats, labels, idxA, idxB):\n",
    "    \"\"\"\n",
    "    Visualizes the decision boundaries of a classifier trained on two features of the dataset.\n",
    "    Args:\n",
    "        classifier: linear classifier trained on 2 features.\n",
    "        feats: features to be used for visualization.\n",
    "        labels: labels to be used for visualization.\n",
    "        idxA & idxB: indices of the features to be used for visualization. \n",
    "    \"\"\"\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "\n",
    "    ys = np.sort(np.unique(labels))\n",
    "    y_ind = np.searchsorted(ys, labels)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    x0, x1 = feats[:, 0], feats[:, 1]\n",
    "    all_feats = np.concatenate((x0, x1))\n",
    "    pad = np.percentile(all_feats, 60)\n",
    "\n",
    "    x_min, x_max = x0.min() - pad, x0.max() + pad\n",
    "    y_min, y_max = x1.min() - pad, x1.max() + pad\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "    preds = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    preds = preds.reshape(xx.shape)\n",
    "\n",
    "    lut = np.sort(np.unique(labels)) \n",
    "    ind = np.searchsorted(lut,preds)\n",
    "\n",
    "    markers = [\"o\", \"v\", \"P\", \"X\", \"s\", \"p\", \"h\", \"d\"]\n",
    "    ax.contourf(xx, yy, preds, cmap=plt.cm.Pastel1, alpha=0.8)\n",
    "    for i in range(len(lut)):\n",
    "        ax.scatter(x0[y_ind == i], x1[y_ind == i], color=plt.cm.jet(i/len(lut)), s=50, edgecolors='k', marker=markers[i])\n",
    "\n",
    "    ax.set_xlabel(f'Feature {idxA}')\n",
    "    ax.set_ylabel(f'Feature {idxB}')\n",
    "    ax.set_title('Decision Boundary')\n",
    "\n",
    "    handles = []\n",
    "    markers = [\"o\", \"v\", \"P\", \"X\", \"s\", \"p\", \"h\", \"d\"]\n",
    "    handles = [plt.plot([],[],color=plt.cm.jet(i/len(lut)), ls=\"\", marker=markers[i])[0] for i in range(len(lut))]\n",
    "    labels = [f'Class {i}' for i in lut]\n",
    "    ax.legend(handles, labels, loc='upper right')\n",
    "    plt.show()\n",
    "    # plt.savefig('results/decision_boundary.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c62035-5332-4a46-a378-71a1746792dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset='taiji', verbose=False, subject_index=3):\n",
    "    '''\n",
    "    Loads the taiji dataset.\n",
    "    Args:\n",
    "        dataset: name of the dataset to load. Currently only taiji is supported.\n",
    "        verbose: print dataset information if True.\n",
    "        subject_index: subject index to use for LOSO. The subject with this index will be used for testing. \n",
    "\n",
    "    Returns (all numpy arrays):\n",
    "        train_feats: training features.\n",
    "        train_labels: training labels.\n",
    "        test_feats: testing features.\n",
    "        test_labels: testing labels.\n",
    "    '''\n",
    "\n",
    "    if dataset == 'taiji':\n",
    "        labels = np.loadtxt(\"../P1/data/taiji/taiji_labels.csv\", delimiter=\",\", dtype=int)\n",
    "        person_idxs = np.loadtxt(\"../P1/data/taiji/taiji_person_idx.csv\", delimiter=\",\", dtype=int)\n",
    "        feats = np.loadtxt(\"../P1/data/taiji/taiji_quat.csv\", delimiter=\",\", dtype=float)\n",
    "\n",
    "        # Combine repeated positions\n",
    "        labels[labels == 4] = 2\n",
    "        labels[labels == 8] = 6\n",
    "\n",
    "        # Remove static dimensions. Get mask of all features with zero variance\n",
    "        feature_mask = np.var(feats, axis=1) > 0\n",
    "\n",
    "        # Train mask\n",
    "        train_mask = person_idxs != subject_index\n",
    "\n",
    "        train_feats = feats[feature_mask, :][:, train_mask].T\n",
    "        train_labels = labels[train_mask].astype(int)\n",
    "        test_feats = feats[feature_mask, :][:, ~train_mask].T\n",
    "        test_labels = labels[~train_mask].astype(int)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(f'{dataset} Dataset Loaded')\n",
    "        print(f'\\t# of Classes: {len(np.unique(train_labels))}')\n",
    "        print(f'\\t# of Features: {train_feats.shape[1]}')\n",
    "        print(f'\\t# of Training Samples: {train_feats.shape[0]}')\n",
    "        print('\\t# per Class in Train Dataset:')\n",
    "        for cls in np.unique(train_labels):\n",
    "            print (f'\\t\\tClass {cls}: {np.sum(train_labels == cls)}')\n",
    "        print(f'\\t# of Testing Samples: {test_feats.shape[0]}')\n",
    "        print('\\t# per Class in Test Dataset:')\n",
    "        for clas in np.unique(test_labels):\n",
    "            print(f'\\t\\tClass {clas}: {np.sum(test_labels == clas)}')\n",
    "        \n",
    "    return train_feats, train_labels, test_feats, test_labels\n",
    "\n",
    "def plot_conf_mats(dataset, **kwargs):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrices for the training and testing data.\n",
    "    Args:\n",
    "        dataset: name of the dataset.\n",
    "        train_labels: training labels.\n",
    "        pred_train_labels: predicted training labels.\n",
    "        test_labels: testing labels.\n",
    "        pred_test_labels: predicted testing labels.\n",
    "    \"\"\"\n",
    "\n",
    "    train_labels = kwargs['train_labels']\n",
    "    pred_train_labels = kwargs['pred_train_labels']\n",
    "    test_labels = kwargs['test_labels']\n",
    "    pred_test_labels = kwargs['pred_test_labels']\n",
    "\n",
    "    train_confusion = confusion_matrix(train_labels, pred_train_labels)\n",
    "    test_confusion = confusion_matrix(test_labels, pred_test_labels)\n",
    "\n",
    "    # Plot the confusion matrices as seperate figures\n",
    "    fig, ax = plt.subplots()\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion, display_labels=np.unique(train_labels))\n",
    "    disp.plot(ax=ax, xticks_rotation='vertical')\n",
    "    ax.set_title('Training Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results/{dataset}_train_confusion.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion, display_labels=np.unique(test_labels))\n",
    "    disp.plot(ax=ax, xticks_rotation='vertical')\n",
    "    ax.set_title('Testing Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(f'results/{dataset}_test_confusion.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "def example_decision_boundary(dataset='taiji', indices=[0, 6]):\n",
    "    \"\"\"\n",
    "    An example of how to visualize the decision boundary of a classifier.\n",
    "    \"\"\"\n",
    "    train_feats, train_labels, test_feats, test_labels = load_dataset(dataset=dataset)\n",
    "\n",
    "    dc_train_feats = train_feats[:, indices]\n",
    "    dc_test_feats = test_feats[:, indices]\n",
    "\n",
    "    # Example Linear Discriminant Analysis classifier with sklearn's implemenetation\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(dc_train_feats, train_labels)\n",
    "\n",
    "    # Visualize the decision boundary\n",
    "    viz_desc_bounds(clf, dc_test_feats, test_labels, indices[0], indices[1])\n",
    "\n",
    "def example_classification(dataset='taiji'):\n",
    "    \"\"\"\n",
    "    An example of performing classification. Except you will need to first project the data.\n",
    "    \"\"\"\n",
    "    train_feats, train_labels, test_feats, test_labels = load_dataset(dataset=dataset)\n",
    "\n",
    "    # Example Linear Discriminant Analysis classifier with sklearn's implemenetation\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(train_feats, train_labels)\n",
    "\n",
    "    # Predict the labels of the training and testing data\n",
    "    pred_train_labels = clf.predict(train_feats)\n",
    "    pred_test_labels = clf.predict(test_feats)\n",
    "\n",
    "    # Get statistics\n",
    "    plot_conf_mats(dataset, train_labels=train_labels, pred_train_labels=pred_train_labels, test_labels=test_labels, pred_test_labels=pred_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4628a0c-3fe7-4ef2-ad64-7fd2bfe1d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_decision_boundary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f81cb2-0150-422e-a38a-dc44b56e382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_classification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
